# ğŸ§  The WoI Encoder â€” My Way  
**Words of Importance: Semantic Transparency for AI**

> *"Itâ€™s not overfitting. Itâ€™s overperforming." â€“ Johan Wayâ„¢*

---

## ğŸ“ Repository Structure

```
WoI/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ wine_train_clean_v5.jsonl        # Cleaned & curated Kaggle wine dataset (truncated example)
â”‚   â”œâ”€â”€ wine_is_fine.csv                 # Dataset used for WoI demonstration
â”‚   â”œâ”€â”€ vocab_map.json                   # Token â†’ index mapping (truncated)
â”‚
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ quad_encoded_data_flat.json      # Example encoded data (hashed + information weights)
â”‚   â”œâ”€â”€ woi_sweep_accuracy.png           # Accuracy plot for dynamic WoI sweep
â”‚   â”œâ”€â”€ woi_sweep_accuracy.csv           # Results table (WoI vs accuracy)
â”‚
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ Exclusive to business agreements and academic collaborations  
â”‚       under signed NDA on my terms.
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ Proprietary â€“ core algorithms and methods are not included.  
â”‚       Available only under research or licensing collaboration.
â”‚
â”œâ”€â”€ README.md
â”‚
â””â”€â”€ LICENSE  
    Exclusive to business agreements and academic collaborations  
    under signed NDA on my terms.
```

---

## ğŸŒ Overview

The **WoI Encoder (Words of Importance)** is a conceptual framework and experimental encoder designed to *measure the information value of words*.  
It unites **semantic transparency**, **mutual information weighting**, and **encryption-based tokenization** to show that *interpretability* and *compression* can coexist.

Built entirely on a **Mac M3 Air**, using standard Python tooling â€” a demonstration of how far individual innovation can go.

---

## âš—ï¸ Methodology

The encoder transforms text into numerical representations weighted by *Word-level Information (WoI)*.  
Each token is assigned a deterministic yet encrypted hash, allowing controlled reconstruction and mutual information (MI) evaluation.

The system:
- Encodes text via POS-aware hashing  
- Computes normalized MI weights  
- Aggregates signals into interpretable â€œWoI vectorsâ€  
- Demonstrates classification efficiency with minimal compute requirements  

See [`docs/methodology.md`](docs/methodology.md) for a detailed conceptual description.

---

## ğŸ“Š Results

**Example Task:** *Wine Review Classification*  

**Dataset:** `wine_is_fine.csv`  
Curated subset derived from the open *Wine Reviews* dataset on Kaggle:  
ğŸ”— https://www.kaggle.com/datasets/zynicide/wine-reviews

License: Public domain / Creative Commons Attribution (original source).  
Processed and normalized by **Johan Eskils** for semantic encoder benchmarking.

**Model:** Logistic Regression  
**Platform:** Mac M3 Air  

| WoI Threshold | Accuracy |
|---------------|-----------|
| 100 | 0.72 |
| 200 | 0.78 |
| **400** | **0.81** |
| 800 | 0.79 |

ğŸ“ˆ *Figure:* `output/woi_sweep_accuracy.png`  

> â€œNot overfitting â€” overperforming.â€  

---

## ğŸ§  Purpose

WoI represents an ongoing exploration into:
- Low-compute semantic modeling  
- Information-driven interpretability  
- Transparent encoding for AI governance and data security  

It reflects a vision where understanding meaning is measurable â€”  
and where every word counts.

---

## ğŸ›¡ï¸ License

This repository is shared **for demonstration and research purposes only**.  
All source code related to the *core encoding mechanism* is **proprietary** and **not included**.  
Distribution or reuse is subject to signed NDA and collaboration agreement.

Â© 2025 **Johan Eskils** â€” All rights reserved.

---

## âœ‰ï¸ Contact

For collaboration or research inquiries:  
ğŸ”— [linkedin.com/in/johaneskils](https://www.linkedin.com/in/johaneskils)  
ğŸ“§ *Email available upon request*

---

## ğŸ§© Citation

If referencing this work in research or media, please cite:

```
@misc{Eskils2025WoI,
  author = {Johan Eskils},
  title  = {The WoI Encoder â€” Words of Importance},
  year   = {2025},
  note   = {Pre-release concept, ongoing research}
}
```

---

> *â€œSometimes it takes one person to redefine meaning itself.  
> And when LLMs finally speak my languageâ€¦ youâ€™ll know.â€*
